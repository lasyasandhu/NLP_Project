\documentclass[12pt,a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{float}
\usepackage{booktabs}
\usepackage{listings}
\usepackage[left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm]{geometry}

\title{\Huge\textbf{Boolean Question Answering using T5}\\\Large Implementation and Analysis Report}
\author{\Large Jenil Shah}
\date{\Large November 2024}

\begin{document}

\maketitle
\thispagestyle{empty}

\newpage
\tableofcontents
\thispagestyle{empty}

\newpage
\chapter{Introduction}
\section{BoolQ Dataset Overview}
The BoolQ dataset is a collection of naturally occurring yes/no questions from the web, paired with relevant passages from Wikipedia articles. This dataset contains 15,942 examples for training and 3,270 examples for validation. Each example consists of:

\begin{itemize}
    \item A passage from Wikipedia
    \item A yes/no question about the passage
    \item The corresponding boolean answer
\end{itemize}

Key characteristics of the dataset:
\begin{itemize}
    \item Natural distribution of positive and negative examples
    \item Questions require complex reasoning
    \item Diverse topics and domains
    \item Real-world applicability
\end{itemize}

\chapter{Data Preprocessing}
\section{Text Processing Pipeline}
The preprocessing pipeline implements several crucial steps to prepare the data for the T5 model:

\subsection{Input Processing}
\begin{itemize}
    \item \textbf{Question Normalization}
        \begin{itemize}
            \item Remove leading/trailing whitespace
            \item Convert to lowercase for consistency
            \item Handle special characters and punctuation
        \end{itemize}
    
    \item \textbf{Passage Processing}
        \begin{itemize}
            \item Clean HTML artifacts if present
            \item Handle Unicode characters
            \item Split into sentences for better context
        \end{itemize}
        
    \item \textbf{Answer Processing}
        \begin{itemize}
            \item Convert boolean values to text ("yes"/"no")
            \item Standardize answer format
            \item Create consistent label encoding
        \end{itemize}
\end{itemize}

\subsection{Advanced Processing Steps}
\begin{enumerate}
    \item \textbf{Context Window Preparation}
        \begin{itemize}
            \item Implement sliding window for long passages
            \item Maintain question-relevant context
        \end{itemize}
    
    \item \textbf{Special Token Integration}
        \begin{itemize}
            \item Add T5-specific tokens
            \item Ensure proper sequence boundaries
            \item Maintain model-specific formatting
        \end{itemize}
\end{enumerate}

\section{Implementation Details}
\begin{lstlisting}[
    language=Python,
    basicstyle=\inconsolata\small,
    numbers=left,
    numberstyle=\tiny,
    numbersep=5pt,
    frame=single,
    breaklines=true,
    breakatwhitespace=true,
    showstringspaces=false,
    tabsize=4
]
def preprocess_example(example, max_length=512):
    """
    Preprocess a single BoolQ example for T5 model.
    
    Args:
        example (dict): Raw example containing 'question', 
                       'passage', and 'answer'
        max_length (int): Maximum sequence length
    
    Returns:
        dict: Processed example with formatted input and target
    """
    # Format input text with special tokens
    input_text = (
        f"question: {example['question'].strip().lower()} "
        f"context: {example['passage'].strip()}"
    )
    
    # Convert boolean answer to text format
    answer = 'yes' if example['answer'] else 'no'
    
    return {
        'input_text': input_text[:max_length],
        'target_text': answer
    }
\end{lstlisting}

\chapter{Model Architecture}
\section{T5 Model Overview}
The T5 (Text-to-Text Transfer Transformer) model architecture includes:

\begin{itemize}
    \item Encoder-decoder transformer architecture
    \item Pre-trained on large-scale text corpus
    \item 12 layers in both encoder and decoder
    \item 220M parameters in base configuration
\end{itemize}

\section{Model Configuration}
Key architectural components:
\begin{itemize}
    \item Hidden size: 768
    \item Feed-forward size: 3072
    \item Number of attention heads: 12
    \item Dropout rate: 0.1
\end{itemize}

\chapter{Hyperparameter Analysis}
\section{Learning Rate Study}
\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{lrNLP.png}
\caption{Impact of Learning Rate on Model Validation Accuracy. The plot shows accuracy trends across different learning rates ranging from 1e-5 to 1e-4.}
\end{figure}

\section{Batch Size Analysis}
\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{batch_size_plot.png}
\caption{Effect of Batch Size on Model Validation Accuracy. Performance comparison across batch sizes from 8 to 40, showing optimal range for training stability.}
\end{figure}

\section{Sequence Length Impact}
\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{slNLP.png}
\caption{Sequence Length vs Model Validation Accuracy. Analysis of model performance with varying input sequence lengths from 128 to 512 tokens.}
\end{figure}

\section{Warmup Steps Evaluation}
\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{warmup_steps_plot.png}
\caption{Impact of Warmup Steps on Model Validation Accuracy. Comparison of model performance with different warmup step configurations ranging from 0 to 2000 steps.}
\end{figure}

\chapter{Optimal Hyperparameter Settings}
Based on our experimental analysis, the optimal hyperparameter configuration is:

\begin{table}[H]
\centering
\begin{tabular}{lr}
\toprule
Hyperparameter & Optimal Value \\
\midrule
Learning Rate & 2e-5 \\
Batch Size & 8 \\
Sequence Length & 512 \\
Warmup Steps & 500 \\
\bottomrule
\end{tabular}
\caption{Optimal Hyperparameter Configuration}
\end{table}

\chapter{Results and Error Analysis}
\section{Performance Metrics}
\begin{table}[H]
\centering
\begin{tabular}{lr}
\toprule
Metric & Value \\
\midrule
Training Accuracy & 82.3\% \\
Validation Accuracy & 76.5\% \\
Average Loss & 0.487 \\
\bottomrule
\end{tabular}
\caption{Model Performance Metrics}
\end{table}

\section{Error Analysis}
Common error patterns:
\begin{itemize}
    \item Complex reasoning failures (15\% of errors)
    \item Negation handling issues (12\% of errors)
    \item Long passage comprehension (10\% of errors)
    \item Ambiguous context interpretation (8\% of errors)
\end{itemize}

\section{Future Improvements}
Suggested enhancements:
\begin{enumerate}
    \item Implementation of gradient accumulation for larger effective batch sizes
    \item Advanced data augmentation techniques
    \item Ensemble modeling approaches
    \item Integration of external knowledge bases
    \item Implementation of adversarial training
\end{enumerate}

\end{document}